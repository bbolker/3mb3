\documentclass[10pt]{article}
\usepackage{url}
\usepackage{sober}
\usepackage{color}
\usepackage{times}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{fullpage}
\newcommand{\qq}[1]{\color{blue} #1 \color{black}}
\title{Multivariate linear discrete-time deterministic models (1)}
\author{Ben Bolker and Steve Walker}
\date{}
\begin{document}
\maketitle

\enlargethispage{20pt}
\thispagestyle{empty}
\begin{multicols}{2}


<<include=FALSE>>=
knitr::opts_chunk$set(height=2.7,fig.width=3)
@

 \setkeys{Gin}{width=3in} 

\section*{Annual plant example}

Parameters: seed production $\gamma$, overwinter survival $\sigma$,
first-year germination $\alpha$, second-year germination $\beta$.
Homogeneous second-order linear model: $N(t)=\gamma \alpha \sigma N(t-1)
+ \gamma \sigma^2 (1-\alpha) \beta N(t-2)$.

For homogeneous linear equations of any order, 0 is an equilibrium.
Find other eq. by (1) plugging in trial solution $C \lambda^t$; (2) dividing through by $C \lambda^{n-1}$; (3) solve for $\lambda$ by finding roots of \emph{characteristic equation}; (4) possibly plugging in initial conditions ($N(0)$, $N(-1)$, ...) to solve for constants $c_i$ in particular solution $N(t) = \sum_i c_i \lambda_i^t$ (ignoring repeated-root case).

In this case with $a=\gamma \alpha \sigma$, $b=\gamma \sigma^2 (1-\alpha) \beta$, we have $\lambda^2-a\lambda-b = 0 \to \lambda= (a \pm \sqrt{a^2+4b})/2$. Given that $\lambda>1 \leftrightarrow a+b>1$, population grows if $\gamma > 1/(\alpha \sigma + \beta (1-\alpha) \sigma^2)$.

Or we can set this up as a matrix equation:
$$
\left( \begin{array}{c} P(t+1) \\ S(t+1) \end{array}\right) 
=  \left(
\begin{array}{cc}
\gamma \alpha \sigma & \sigma \beta \\
\gamma \sigma (1-\alpha)  & 0
\end{array}
\right)
\left( \begin{array}{c} P(t) \\ S(t) \end{array}\right) 
$$
  
Basic model: $\bm x(t+1) = \bm M \bm x(t)$. 

Alternative case,
juvenile/adult model: fractions $\{s_J,s_A\}$ of juveniles and adults
survive; adults have $f$ offspring each (on average); surviving
juveniles become adults.  So $A(t+1)=s_AA(t)+s_JJ(t)$, $J(t+1)=fA(t)$
\emph{or} $A(t+1)=s_A A(t) + s_JfA(t-1)$. This can be written as a
 matrix equation, 
 \begin{equation*}
   \begin{bmatrix}
     J(t+1) \\ A(t+1)
   \end{bmatrix} = 
   \begin{bmatrix}
     0 & f \\ s_J & s_A
   \end{bmatrix} 
   \begin{bmatrix}
     J(t) \\ A(t)
   \end{bmatrix}
 \end{equation*}
    
\section*{Fixed points}

$\bm x{\star}$ is a fixed point if $\bm x{\star} = \bm M \bm
x{\star}$, $\bm 0 = (\bm M - \bm I) \bm x{\star}$ where $\bm I$ is the
identity matrix. The null space of $\bm M - \bm I$ has all the fixed
points. If $\bm M - \bm I$ is invertible, we find $\bm 0 = (\bm M -
\bm I )^{-1}(\bm M - \bm I) \bm x{\star}$, which implies $\bm 0 = \bm
I \bm x{\star}$, or $\bm x{\star} = \bm 0$. However, if $\bm M - \bm
I$ is not invertible, there is an $n-r$ dimensional space of
fixed-points, where $n$ is the number of rows/columns in $\bm M - \bm
I$ and $r$ is the rank of that matrix. A matrix is invertible if its determinant is non-zero. For example, the
determinant of the juvenile-adult model is $-fs_J$, which is not zero
and so the only fixed point is at the origin.

In Python, the rank, inverse, and determinant of a matrix
\texttt{B} are given by \texttt{numpy.linalg.matrix\_rank(B)}, 
\texttt{numpy.linalg.inv(B)}, and
\texttt{numpy.linalg.det(B)}.

\section*{Time-dependent solution}

%% A(t+2) = s_A A(t+1) + s_J fA(t)
%%        = s_A(s_A A(t)+ s_J f A(t-1)) +s_J fA(t) 
%%        = (s^2_A +s_J f)A(t) + s_A s_J f A(t-1)
%% A(t+3) = s_A A(t+2) + s_J fA(t+1)
%%        = (s^3_A + s_A s_J f) A(t) + s_A^2 s_J f A(t-1) + s_J f (s_A A(t) + s_J f A(t-1))
%%        = (s^3_A + 2 s_A s_J f) A(t) + (s_A^2 s_J f + s_J^2 f) A(t-1)
%%  ... ugh ...
Four approaches:
\begin{description}
\item[recursion] $\bm x(1) = \bm M \bm x(0)$, then $\bm x(2) =
  \bm{AA} \bm x(0)$, and in general $\bm x(t) = \underbrace{\bm M
    ... \bm M}_{t-\text{times}} \bm x(0)$.
  \item[matrix powers] We can define matrix powers
(in Python: \texttt{numpy.linalg.matrix\_power}), so that $\bm x(t) =
    \bm M^t \bm x(0)$. This is efficient but doesn't provide much
    insight. 
  \item[diagonalization] Gain insight by \emph{diagonalizing} $\bm M =
    \bm{SDS}^{-1}$, where $\bm S$ is a matrix whose columns are the
    eigenvectors of $\bm M$ and $\bm D$ is a matrix with eigenvalues
    on the diagonal and zeros everywhere else. Substituting into the
    matrix power equation, $\bm x(t) = \left(\bm S \bm D \bm
      S^{-1}\right)^t \bm x(0) = \underbrace{\bm S \bm D \bm S^{-1}
      \bm S \bm D \bm S^{-1} ...  \bm S \bm D \bm
      S^{-1}}_{t-\text{times}} \bm x(0) = \underbrace{\bm S \bm D^{t}
      \bm S^{-1}}_{t-\text{times}} \bm x(0)$, because the $\bm S^{-1}
    \bm S$ terms cancel. 
   \item[series] Let $\bm c = \bm S^{-1} \bm x(0)$. This
    allows us to write $\bm x(t) = \sum_i c_i \lambda_i^t \bm v_i$, where
    $c_i$, $\lambda_i$, and $\bm v_i$ is the $i$th element of $\bm c$,
    eigenvalue, and eigenvector respectively. This form also lets us
    see the importance of the dominant eigenvalue (i.e. eigenvalue
    with largest absolute value) -- because all the eigenvalues get
    raised to the power of time, as time increases all other terms
    except for the dominant become neglible. Therefore, for $t$
    sufficiently large, $\bm x(t) \approx c_1 \lambda_1^t v_1$, where $\lambda_1$
    is the dominant eigenvalue. \qq{What happens when $\lambda_1 = 1$?}
    \qq{What happens when $\lambda_1 = \lambda_2$? Try it out in Python}.
   \item[change of variables] Let 
     $\bm y(t) = \bm S^{-1} \bm x(t)$. Then the model becomes 
     $\bm y(t+1) = \bm D \bm y(t)$. But
     since $\bm D$ is diagonal, this model is exceptionally
     simple. It is actually just a bunch of decoupled univariate
     models \qq{(Why?)}and you know how to handle those.
\end{description}

\subsection*{Eigen-tips (mostly for the 2 by 2 case)}

If $\bm M = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} &
  a_{22} \end{bmatrix}$, determinant is $\Delta = a_{11}a_{22} -
  a_{12}a_{21}$, trace is $T = a_{11} + a_{22}$, and eigenvalues obey
  $\lambda_1 + \lambda_2 = T$ and $\lambda_1 \lambda_2 = \Delta$. This leads to the
  characteristic polynomial $\lambda_i^2 - T\lambda_i + \Delta$. And so the
  eigenvalues obey $\lambda_i = (T \pm \sqrt{T^2 -
      4\Delta})/2$. Finally, if $\bm v_i$ and $\lambda_i$ are an
  eigenvector/eigenvalue pair for $\bm M$, then $\bm M \bm v_i = \lambda_i
  \bm v_i$ (i.e. a matrix and a single scalar value do the same thing
  to an eigenvector!).
  
  Example: for the juvenile-adult model, we have $\lambda_i = (s_A
    \pm \sqrt{s_A^2 + 4s_Jf})/2$. For each eigenvalue, solve $\begin{bmatrix}
     0 & f \\ s_J & s_A \end{bmatrix} \begin{bmatrix} v_1 \\
     v_2 \end{bmatrix} = d \begin{bmatrix} v_1 \\
     v_2 \end{bmatrix}$ to find the eigenvectors. For the dominant
   eigenvalue, this is
   \begin{equation*}
     \begin{split}
       fv_2 = & \frac{s_A + \sqrt{s_A^2 + 4s_Jf}}{2} v_1 \\
       s_Jv_1 + s_Av_2 = & \frac{s_A + \sqrt{s_A^2 + 4s_Jf}}{2} v_2
     \end{split}
   \end{equation*}
   Could keep going but you get the idea. \qq{Simplify this system.}
   \qq{Do the same for the other eigenvalue.} \qq{Write down a
     time-dependent solution for this model with your computations.}
   \qq{What are the conditions for stability of the fixed point at the
     origin?}
   
\section*{Affine model}

Multivariate bucket/line-up: $\bm x(t+1) = \bm b + \bm M \bm
x(t)$. For fixed points solve $\bm x{\star} = \bm b + \bm M \bm
x{\star}$. If $\bm M - \bm I$ is invertible, then the solution is 
$\bm x{\star} = (\bm I - \bm M)^{-1} \bm b$ \qq{Note the similarity
to the affine case for univariate models \ldots}. 
Same stability conditions as
in the linear case. \qq{Can you reparameterize this model such that
  the fixed point is a parameter? It would be nice to just read off
  the fixed point woudn't it?}
%$\bm x(t+1) = \bm M (\bm x(t) - \hat{\bm x})$

\end{multicols}
\end{document}
